# Time-stamp: <2024-09-15 07:31:04 angelv> 

!!!!!!!!!!!!! DO NOT UPDATE
!!!!!!!!!!!!!  all user documentation now goes to index.rst
!!!!!!!!!!!!!  just take the info in :noexport: sections
!!!!!!!!!!!!!  and incorporate them in index.rst

* Submit files (HowTo)                                             :noexport:

** How to ... add requirements on the target machines where my jobs will be run? 

If your program has some limitations (memory, disk, libraries, etc.) and cannot
run in all machines, you can use =requirements= command to tell HTCondor what
those limitations are so it can execute your jobs only on machines that satisfy
those requirements. If you try next command =condor_status -long <your_machine>=
in your shell, it will list all the parameters that HTCondor has about each slot
of your machine, and most of those parameters can be used to add
requirements. Conditions and expressions could be as complex as needed, there is
a number of *operators, predefined functions, etc.* that can be used. For
memory, disk and CPUs you can also use =request_memory=, =request_disk= and
=request_cpus=, respectively.

For example, if your program needs at least 1.5 GB of RAM and 5 GB of free space
in disk, and due to library dependencies it can only run on machines with Linux
Fedora17 or above, add next commands in your submit file:

 =request_disk=   = 5 GB
 =request_memory= = 1.5 GB
 =requirements=   = (UtsnameSysname == "Linux") && (OpSysName == "Fedora") && (OpSysMajorVer >= 17)

Be careful when specifying the values since the default unit for =request_disk=
is KB and MB for =request_memory=. It is much better to always specify the unit
(=KB= or =K=, =MB= or =M=, =GB= or =G=, =TB= or =T=).


*Caution!*: Be careful when choosing your restrictions, using them will reduce
the amount of available slots for your jobs so it will be more difficult to
execute them. Also check that you are asking for restrictions that can be
satisfied by our current machines, or your jobs will stay always in idle status
(you can check the reasons why a job is idle using =condor_q -analyze
<job.id>=). Before adding a requirement, always check if there are enough slots
that satisfy it. For instance, to see which slots satisfy the requirements of
this example, use next command (you can add flag =-avail= to see only the slots
that could execute your job at this moment):

 [...]$ =condor_status= -constraint '(UtsnameSysname == "Linux") && (OpSysName == "Fedora")  
                    && (OpSysMajorVer >= 17) && (Memory > 1536)  && (Disk >= 5120000)'

If you already know which machines are not able to run your application, you can
force HTCondor to avoid them... or the opposite: run your application only on
some machines (see *HOWTOs.#howto_failing* where this is explained).

** How to ... add preferences on the target machines where my jobs will be run?  

Preferences are similar to requirements, but they do not limit the machines (you
can use both preferences and requirements in the same submit file). HTCondor
will try to satisfy your preferences when possible, assigning a rank to the
available machines and choosing those with higher value. For example, we would
like to use slots with at least 4GB of RAM if they are available and the more
available disk space, the better. Then commands to add should be the following
ones:

 =Rank=  = Disk && (Memory >= 4096)

Rank is evaluated as a float point expression, and always higher values are the
better ones. Then, you can do arithmetic operations to emphasize some
parameters. For example, the first expression will consider the floating point
speed but will give more importance to run on machines with my same Operating
System, while the second expression will choose machines with higher values of
RAM, and those with also more than 100GB of disk will have 200 extra ''points'':

 =Rank=  = kflops + (1000000 * (TARGET.OpSysAndVer == MY.OpSysAndVer))
 =Rank=  = Memory + (200 * (Disk >= 102400))   


** How to ... get/set environment variables?  

If you application needs them, use the =getenv= command and HTCondor will create
a copy of your environment variables {+at the submitting time+} so they will be
available for your program on the target machine. Also you can create/modify
environment variables if needed with the =environment= command. The environment
variables can be also used in the submit file using the =ENV= command.

For example, add next commands if you want that your executable can access your
environment at the submitting time, then set variables called =working_dir= and
=data_dir= pointing to some directories, and finally create a macro called
=home_dir= that contains your home directory to be used in your submit file:

 =getenv=        = True
 =environment=   = "working_dir=/path/to/some/place data_dir=/path/to/data"
 home_dir      = $ENV(HOME)

If you want to run your python program using HTCondor, you might need to define
some environment variables, please, read this FAQ
*HOWTOs.CondorFAQs#python-fedora*.


** How to ... specify the priority of your jobs?  

HTCondor uses two different types of priorities: *job priority* (which of your
jobs will run first) and *users priority* (which users will run their jobs and
how many of them).

[+*Job priority*+]

If some of your jobs/clusters are more important than others and you want to
 execute them first, you can use the =priority= command to assign them a
 priority (the higher the value, the higher priority). For instance, if you want
 to execute first the last jobs of a cluster (reverse order), you can use next

 command: =priority= = $(Process)

Remember that after submitting your jobs, you can set or change their priority
using the shell command =condor_prio=.

[+*Users priority*+]

Whenever your jobs are being executed, your user priority is decreased (the more
jobs are executed, the faster you lose priority). Users with best priority will
run more jobs and they will begin sooner, so if your jobs are not so important
or the queue is empty, you can use =nice_user= command to run them without
wasting your priority. If you set this command to =True=, your jobs will be
executed by a ''fake user'' with very low priority, so you will save your real
priority (but it is likely your jobs will not be executed unless the queue is
almost empty).

 =nice_user=     = True

Those jobs will be run with user =nice-user.<your_user>= and they will not
change your user's priority (you can use shell command =condor_userprio
-allusers= to see your and other users' priority).  

Remember that using =condor_qedit= command you can change the attributes of your
jobs after submitting them (see this FAQ *HOWTOs.CondorFAQs#ch_submit*). We can
use this command to change the status of =NiceUser= attribute depending on how
many slots are free (if there are many free slots, then we can set our jobs as
''nice'' to run them without affecting our priority, or the opposite, setting
=NiceUser= to =false= when there are no free slots). For instance, use next
commands to set all jobs belonging to Cluster ID =1234=:

  =[...]$ condor_q 1234 -af ProcId NiceUser=  ''#Check current status''
         0 false
         1 false
         ...

  =[...]$ condor_qedit 1234 NiceUser True=
         Set attribute "NiceUser".

  =[...]$ condor_q 1234 -af ProcId NiceUser=  ''#Check current status''
         0 true
         1 true
         ...

If user priority is a critical factor to you, you may want to periodically check
the queue to change the =NiceUser= attribute according to the current status,
setting it to =True= when you are the only active user or there is a large
number of available slots, and set it to =False= when there are more active
users or a few available slots. In order to simplify this process, we have
developed a script that automatically performs those operations, you only need
to specify your username or a particular =clusterID= (and optionally a minimum
number of available slots) and it will change the =NiceUser= attribute to save
your real priority as much as possible. You can copy the script from
=/net/vial/scratch/adorta/htcondor_files/htcondor_niceuser.sh= and use it (or
modify it) whenever you want. You can even periodically execute it using
=crontab= (but please, do NOT run it too often to avoid overloading the system,
every 30 or 60 minutes is fine). Run it with no arguments to get the description
and syntax.

*Notes*: 

+ You can use =condor_qedit *-constraint* ...= to change the attributes of only
  some of your jobs.
+ Condor can evaluate the attributes only when jobs begin to run, so new values
  may not affect the currently running jobs at the time of using =condor_qedit=,
  but they will be valid in jobs that begin to run after using the command.


** How to ... deal with jobs that fail?  

Sometimes jobs fail because there are problems when executing your program. It
could happen that the problem is not in your program, but in the machine that
executed it (a missing or misconfigured application, a library with a different
version from the one you need, etc.). Then you should identify those problematic
machines and use =requirements= commands in your submit file in order to block
them, as is explained in this FAQ *HOWTOs.CondorFAQs#blackholes*. For example,
to block machines with names =piston= and =loro= use only one of the next
commands (both are equivalent):

 =requirements= = ((UtsnameNodename =!= "piston") && (UtsnameNodename =!= "loro"))  
 =requirements= = *!*=stringListMember=(UtsnameNodename, "piston,loro")

You can also block all machines that satisfy a pattern. For instance, to avoid
executing your jobs on those machines with names beginning with "k", "c" and
"l", add next lines (you can specify more complex patterns using the
*predefined functions and macros*:

  letter       = =substr=(toLower(Target.Machine),0,1)
  =requirements= = *!*=stringListMember=($(letter), "k,c,l")

Sometimes it is better to specify a list of machines where your application can
run (and avoid any other that is not in that list). For that purpose, just use
previous expressions after negating them with an ''exclamation mark'' "*!*" (or
remove it if they were already negated).

After avoiding machines that are not able to run your program, you should submit
again your jobs. But, please, execute {+only+} those jobs that failed (check
this FAQ *CondorFAQs#repeat* to see how), do not execute again jobs that were
already correctly executed to avoid wasting time and resources. For instance,
add next command to only execute jobs with Process ID =0=, =13=, =25= and those
from =37= to =44=:

   =noop_job= = *!*=( stringListMember=("$(Process)","0,13,25") || (($(Process) >= 37) && ($(Process) <= 44)) =)=

*Note*: =noop_job= will ''not'' execute those jobs where the condition is
=True=. Therefore, if you want to specify a list of jobs ''to be executed'', you
need to ''negate'' your expression adding an exclamation mark at the beginning:
=noop_job = *!*(...)=. On the other hand, if you want to specify a list of jobs
that should ''not'' be executed, then use the expression without negating it.

Jobs that are not executed may stay in the queue with =Complete= status (when
using =condor_q= you will see that =ST= column is =C=). To remove all =C= jobs
from the queue, try next command in your shell (use the second one to remove
{+only+} =Complete= jobs that belongs to cluster =XXX=):

  =condor_rm= -constraint 'JobStatus == 4'
  =condor_rm= -constraint 'JobStatus == 4 && clusterID == XXX'

Also, it could be interesting to avoid the *black holes*: suppose that each of
your jobs needs hours to finish, but they fail in an specific machine after a
few minutes of execution time. That means that machine will be idle every few
minutes, ready to accept another of your jobs, that will also fail, and this
process may repeat again and again... sometimes a failing machine could even
execute almost all your jobs... That is known as ''black hole''. To avoid it, we
can force HTCondor to change machines when sending jobs. For that purpose add
these lines to your submit file:

  ''#Avoid black holes: send to different machines''
  =job_machine_attrs= = Machine  
  =job_machine_attrs_history_length= = 5           
  =requirements= = $(requirements) && (target.machine =!= MachineAttrMachine1) && (target.machine =!= MachineAttrMachine2)


When there are problems with your jobs, you should receive an email with an
error and some related information (if it was not disabled using =notification=
command as explained above) and the job will leave the queue. You can change
this behavior with =on_exit_hold= and/or =on_exit_remove= commands, forcing
HTCondor to keep that job in the queue with status ''on hold'' or even as
''idle'' so it will be executed again:

(:table border=1 cellpadding=5 cellspacing=0 width=70% align=center:)
(:cell align=center valign=middle :) *Command* 
(:cell align=center valign=middle :) * True *
(:cell align=center valign=middle :) * False *
(:cellnr align=center valign=middle :) =on_exit_hold=
(:cell align=center valign=middle :) Stay in the queue with ''on hold'' status
(:cell align=center valign=middle :) Leave the queue
(:cellnr align=center valign=middle :) =on_exit_remove=
(:cell align=center valign=middle :) Leave the queue
(:cell align=center valign=middle :)  Stay in the queue with ''idle'' status (it can be executed again)
(:tableend:)
\\\

Last commands will be evaluated when jobs are ready to exit the queue, but you
can force a periodic evaluation (using a configurable time) with commands like
=periodic_hold=, =periodic_remove=, =periodic_release=, etc., and then decide if
you want to hold/remove/release them according to your conditions. There are
also some other commands to add a ''reason'' and/or a ''subcode'' when
holding/removing/releasing these jobs. On the other hand, you can force your
jobs to exit the queue when they satisfy a given condition using =noop_job=, or
they stay in the queue even after their completion using =leave_in_queue=
command (those jobs will stay in the queue with =Complete= status till you
remove them using shell command =condor_rm=).


In the
http://research.cs.wisc.edu/htcondor/manual/v8.6/condor_submit.html#condor-submit-on-exit-hold
official HTCondor documentation there are some examples about how to use these
commands (all valid =JobStatus= could be displayed using shell command:
=condor_q -help status=):

+ With the next command, if the job exits after less than an hour (3600
  seconds), it will be placed on hold and an e-mail notification sent, instead
  of being allowed to leave the queue:
   
=on_exit_hold= = ((CurrentTime - JobStartDate) < 3600)

+ Next expression lets the job leave the queue if the job was not killed by a
  signal or if it was killed by a signal other than 11, representing
  segmentation fault in this example. So, if it exited due to signal 11, it will
  stay in the job queue. In any other case of the job exiting, the job will
  leave the queue as it normally would have done.

   =on_exit_remove= = ((ExitBySignal == False) || (ExitSignal != 11))

+ With next command, if the job was killed by a signal or exited with a non-zero
  exit status, HTCondor would leave the job in the queue to run again:

   =on_exit_remove= = ((ExitBySignal == False) && (ExitCode == 0))

+ Use the following command to hold jobs that have been executing (=JobStatus ==
  2=) for more than 2 hours (by default, all periodic checks are performed every
  5 minutes. Please, contact us if you want a shorter period):

   =periodic_hold= = ((JobStatus == 2) && (time() - EnteredCurrentStatus) >  7200)

+ The following command is used to remove all ''completed'' (=JobStatus == 4=)
  jobs 15 minutes after their completion:

   =periodic_remove= = ((JobStatus == 4) && (time() - EnteredCurrentStatus) >  900)

+ Next command will assign again the ''idle'' status to ''on hold'' (=JobStatus
  == 5=) jobs 30 min. after they were held:

   =periodic_release= = ((JobStatus == 5) && (time() - EnteredCurrentStatus) >  1800)

[-*IMPORTANT*: =periodic_release= command is useful when your program is
correct, but it fails in specific machines and gets the ''on hold'' status. If
that happens, this command will allow HTCondor to periodically release those
jobs so they can be executed on other machines. But {+use this command with
caution+}: if there are problems in your program and/or data, then your
application could be indefinitely held and released, what means a big waste of
resources (CPU time, network used in file transferring, etc.) and inconveniences
for other users, be careful! (you can always remove your jobs using =condor_rm=
command in your shell).-]


When using =periodic_remove= or =periodic_hold= HTCondor submit commands,
running jobs that satisfy the condition(s) will be killed and all files on
remote machines will be deleted. Sometimes you want to get some of the output
files that have been created on the remote machine, maybe your program is a
simulation that does not converge for some sets of inputs so it never ends, but
it still produces valid data and you want to get the output files. In those
cases, do not use the mentioned submit commands because you will lose the output
files, and use instead utilities like =timeout= in order to limit the time that
your application can be running. When using this linux command, you specify the
maximum time your program can run, and once it reaches that limit, it will be
automatically killed. Then HTCondor will detect your program has finished and it
will copy back the output files to your machine as you specified. Next example
will show how to limit the execution of your program up to 30 minutes:

  ''# Some common commands above...''
  ...

  ''# Max running time (in seconds)''
  MAX_TIME = 30 * 60

  ''# Your executable and arguments''
  MY_EXEC = your_exec
  MY_ARGS = "your_arg1 your_arg2"

  ''# If your executable is not a system command, do not forget to transfer it!''
  =transfer_input_files= = your_inputs,$(MY_EXEC)
  ''# By default all new and modified files will be copied. Uncomment next line to indicate only specific output files''
  ''#=transfer_output_files= = your_outputs''

  =executable=          = /bin/timeout
  ''# Since timeout is a system command, we do not need to copy it to remote machines''
  =transfer_executable= = False
  =arguments=           = "$INT(MAX_TIME) $(MY_EXEC) $(MY_ARGS)"

  =queue= ...




** How to ... do some complex operations in my submit file?  

If you need to do some special operations in your submit file like evaluating
expressions, manipulating strings or lists, etc. you can use the *predefined
functions* and some *special macros* that are available in HTCondor. They are
specially useful when defining conditions used in commands like =requirements=,
=rank=, =on_exit_hold=, =noop_job=, etc. since they will allow you to modify the
attributes received from the remote machines and adapt them to your needs. We
have used some of these predefined functions in our examples, but there are many
others that could be used:

+ evaluate expressions: =eval()=, ... 
+ flow control: = ifThenElse()=, ...
+ manipulate strings : =size()=, =strcat()=, =substr()=, =strcmp()=, ... 
+ manipulate lists: =stringListSize()=, =stringListSum()=, =stringListMember()=, ...
+ manipulate numbers: =round()=, =floor()=, =ceiling()=, =pow()=, ...
+ check and modify types: =isReal()=, =isError()=, =int()=, =real()=...
+ work with times: =time()=, =formatTime()=, =interval()=, ...
+ random: =random()=, =$RANDOM_CHOICE()=, =$RANDOM_INTEGER()=, ...
+ etc. 

Check the documentation to see the complete list of *predefined functions*, and
also the *special macros*.

** How to ... work with nested loops?  

You can use =$(Process)= macro to simulate simple loops in the submit file and
use the iterator to specify your arguments, input files, etc. However, sometimes
simple loops are not enough and nested loops are needed. For example, assume you
need to run your program with the arguments expressed in the next pseudocode:

 MAX_I = 8
 MAX_J = 5

 =for= (i = 0; i < MAX_I; i++)
   =for= (j = 0; j < MAX_J; j++)
     ./myprogram -var1==i= -var2==j=

To simulate these 2 nested loops, you will need to use next macros in your
HTCondor submit file:

 MAX_I = 8 
 MAX_J = 5
 N = MAX_I * MAX_J
 ...    
 I = ($(Process) / $(MAX_J))
 J = ($(Process) % $(MAX_J))
 ...
 =executable= = myprogram
 =arguments=  = "-var1=$=INT=(I) -var2=$=INT=(J)"
 =queue= $(N)

Last code will produce a nested loop where macro =$(I)= will work like the
external iterator with values from =0= to =7=; and =$(J)= will be the internal
iterator with values from =0= to =4=.

\\

If you need to simulate 3 nested loops like the next ones:
 =for= (i = 0; i < MAX_I; i++)
   =for= (j = 0; j < MAX_J; j++)
     =for= (k = 0; k < MAX_K; k++)
       ...

then you can use the following expressions:
 N = $(MAX_I) * $(MAX_J) * $(MAX_K)

 I = ( $(Process) / ($(MAX_K)  * $(MAX_J)))
 J = (($(Process) /  $(MAX_K)) % $(MAX_J))
 K = ( $(Process) %  $(MAX_K))

 =executable= = myprogram
 =arguments=  = "-var1 $=INT=(I) -var2 $=INT=(J) -var3 $=INT=(K)" ...
 =queue= $(N)

** How to ... program my jobs to begin at a predefined time?  

Sometimes you may want to submit your jobs, but those jobs should not begin at
that moment (maybe because they depend on some input data that is automatically
generated at any other time). You can use =deferral_time= command in your submit
file to specify when your jobs should be executed. Time has to be specified in
''Unix epoch time'' (the number of seconds elapsed since 00:00:00 on January 1,
1970, Coordinated Universal Time), but, do not worry, there is a linux command
to get this value:

  =date= --date "MM/DD/YYYY HH:MM:SS" +%s

For instance, we want to run a job on April 23rd, 2016 at 19:25. Then, the first
step is to get the epoch time:

  [...]$ =date= --date "04/23/2016 19:25:00" +%s

Our value is =1461435900=, so we only need to add next command to the submit
file:

  =deferral_time= = 1461435900

Bear in mind that HTCondor will run jobs at that time according to remote
machines, not yours. If there are wrong dates or times in remote machines, then
your jobs could begin at other dates and/or times.

Also you can add expressions, like the next one to run your jobs one hour after
the submission:

  =deferral_time= = (CurrentTime + 3600)

It may happen that your job could not begin exactly at that time (maybe it needs
that some files are transferred and they are not ready yet), and in that case
HTCondor may kill your job because the programmed time has expired and your job
is not already running. To avoid that, you can specify a ''time window'' to
begin the execution, a few minutes should be enough. For instance, add next
command to tell HTCondor that your job could begin up to 3 minutes (180 seconds)
after the programmed time:

  =deferral_window= = 180

*Important:* When you submit your programmed jobs, HTCondor will check which
machines are able to run them and once the match is done, those machines will
wait for the programmed time and will not accept any other jobs (actually, it
will show ''Running'' status while waiting for the programmed time). That means
a considerable loss of resources that should be always avoided. Using
=deferral_prep_time= command we can specify that HTCondor could use those
matched machines till some time before really running your jobs.

Then, add next lines to begin your jobs on April 23rd, 2016 at 19:25, specifying
that they can begin up to 3 minutes after that date and that HTCondor could run
other jobs on the matched machines till one minute before the programmed time:

  =deferral_time=      = 1461435900
  =deferral_window=    = 180
  =deferral_prep_time= = 60

HTCondor also allows you to use more powerful features, like specifying jobs
that will be periodically executed at given times using the ''CronTab
Scheduling'' functionality. Please, read the *Time Scheduling for a Job
Execution* section in the official documentation to get more information.


** How to ... know the attributes of the machines where our jobs are run?  

There is a special macro to get the string attributes of target machines that we
can use in our submit file. In this way, some of the parameters of each machine
where HTCondor executes jobs can be accessed with =$$(parameter)=. Also there
are other special macros, like the one used to print the symbol =$= since it is
reserved by HTCondor: =$(DOLLAR)=.

For example, we want to know the name of each slot and machine where our jobs
were executed, adding =.$.name.$.= to the results of =stdout= and =stderr=. Then
we should use next commands:

 =output=        = myprogram.$(ID).$(DOLLAR).$$(Name).$(DOLLAR).out
 =error=         = myprogram.$(ID).$(DOLLAR).$$(Name).$(DOLLAR).err

Ading those commands in your submit file will create output and error files with
names similar to =$.slot3@xilofon.ll.iac.es.$=.


* FAQs                                                             :noexport:

** Having some troubles with your jobs

*** How can I check that my program is running fine? Can I access to the input/output files in the remote machine? 

There are several ways to check in real-time what is happening on the
remote machine while it executes your jobs, so you can see how results are being
generated and whether they are fine or not. All these methods only work {+when
processes are running+}, remember that you can get the =job_id= using command
=condor_q=, be sure that the job you choose is running with state "*R*" (you can
select them using =condor_q -run=).

A) You can check what your program is printing on the "screen" (=stdout= and
=stderr=) and/or in output files on the remote machine while it executes your
program. To display outputs, use =*condor_tail* <job_id>= command and it will
show the latest lines of the specified output like the linux command =tail= does
(you can also add =-f= option to keep showing new content). For instance, if you
want to check the running job =123.45=, just use next commands:

 [...]$ =condor_tail= 123.45                       ''# Show =stdout= (normal output on screen)''
 [...]$ =condor_tail -f= 123.45                    ''# Show =stdout=, it keeps showing new content''
 [...]$ =condor_tail -no-stdout -stderr= 123.45    ''# Show =stderr= (errors on screen)''
 [...]$ =condor_tail -no-stdout= 123.45 file.out   ''# Show output file =file.out= (*)'' 

[- (*) The output file must be listed in the =transfer_output_files= command in
the submit file.-]

B) You can also establish SSH connections to the remote machines where your jobs
are being executed, using the command =*condor_ssh_to_job* <job_id>= (again,
make sure that the job is running). Once the SSH connection is established, you
will be placed in the directory where your program is being run, so you can
check input and output files to see whether the program is running properly (you
should NOT make any modifications in any file to avoid errors). To open an ssh
connection you only need to specify the jobId, see example for job =123.45=:

 [...]$ =condor_ssh_to_job= 123.45
 
If you need to upload or download files, you can open an =sftp= connection using
flag =-ssh sftp= or you can also use =rsync=, see following examples:

 [...]$ =condor_ssh_to_job -ssh sftp= 123.45
 [...]$ =rsync -v -e condor_ssh_to_job= 123.45:<remote filename> <local directory>

*Important*: {+close ssh connection when you are not using it+}. Jobs with open
connections cannot leave the queue, so they will appear as "running" even if
they are already done, waiting till you close the connection.

C) There is a third method, but it is not recommended: the directory where
Condor executes jobs is usually located in the scratch, so in most cases it will
be directly accessible, you only need to know the name of the machines running
your jobs. Use =condor_q -run= to get these names and then access to the working
directory located in =/net/<remote_machine>/scratch/condor/execute/dir_XXXXX=
(where ''XXXXX'' changes in every execution, but it should be easy to recognize
due to owner's name). Note that this is the default configuration, but some
machines have other configurations and/or you may have no permit to access to
those directories.

*** My submitted jobs are always in Idle status, why do they never run (and what about users' priority)? 

If your jobs are always in Idle status, it may be caused by several
reasons, like restrictions that you have specified in the submit file, a low
user's priority, etc. With =condor_q= command you can find out what the reason
is, just choose one of your idle jobs and use next commands:

  =condor_q -analyze= <job_id>
  =condor_q -better-analyze= <job_id>

Condor will display then some detailed information about machines that rejected
your job (because of your job's requirements or because they are not idle but
being used by their owners), machines that could run your job but are busy
executing other users' jobs, available machines to run your job if any, etc. It
will also display the reason why that job is idle and some suggestions if you
have non-suitable requirements.

Check that information and be sure that your requirements can be satisfied by
some of the current machines (pay attention to the suggestions, they may help a
lot!). For instance, if you ask for slots with more than 6GB of RAM, there are
just few of them and they need to be idle to run Condor jobs, so you may need to
wait for a long while before running there (also check that there are no
impossible values, like asking for machines with 16GB per slot, we have none of
them). Before adding a requirement, always check if there are enough slots that
satisfy it (for example, to see which slots have more than 6GB of RAM, try next
command in your shell: =condor_status -constraint 'Memory > 6144'=. ''Please,
visit *Condor submit file page -> CondorSubmitFile* for more info and
examples.''

You can also get messages like =Reason for last match failure: insufficient
priority=. Bear in mind that Condor executes jobs according to users' priority,
so that message means that Condor is right now executing jobs submitted by users
with a better priority than yours, so you will still have to wait a bit. You can
check yours and other users' priority running =condor_userprio -all -allusers=:
all users begin with a priority value of =0.5=, the best one possible, and once
you begin to run jobs with Condor, it will increase your priority value (that
means worse priority) according to the number of machines you are using and the
consumed time (the more you use Condor's resources, the faster your priority
value will be increased). On the other hand, your priority will be gradually
decreased when you are not using Condor.

If your Condor priority is important for you and you want to run some not urgent
jobs, you can submit them using =nice_user = True= command in your submit file:
those jobs will be run by another user called =nice_user.<your_user>= and they
will not affect your real user's priority. But this new user has an extremely
low priority, so its jobs can stay in the queue for a long while before being
executed (but they can be run very fast if the Condor queue is almost empty).

Besides user's priority, all jobs have also their own priority, and you can
change it to specify whether some jobs are more important than others so they
should be executed first (please, *check this FAQ -> #priority*).

*** Condor have problems with input and/or output files... are my paths wrong? 

If Condor is not able to find your input files, probably your jobs will get
the "''on hold''" status (see *this FAQ -> #held*). It is not needed to place
your input files in any special location, but you need to specify the path to
each file (it could be absolute or relative to the directory where you will do
the submission) and make sure that the path is correct and you have access
permits. Check *this FAQ -> #inputs* to see which commands you can use to
specify the input files.

On the other hand, you also have to specify the output files that will be
generated by your program so Condor can copy them from the remote machines to
your computer. Check *this FAQ -> #outputs* to see which commands you can use
for this purpose.

*** My jobs are 'on hold' status and never finish, what does it mean? 
When there is an error, jobs change their state to "''on hold''". It means
that Condor is expecting an action from the user to continue with those
jobs. Most times the reason to hold jobs are related to permissions or missing
files. A common problem is to specify files that cannot be accessed from other
machines, like those in your home or desktop directories (use Condor commands to
copy files instead), or the destination directory for output files does not
exist or is not reachable, etc. You can check all your held jobs and the reason
for that running next command in your shell: =condor_q -hold=. Once you have
fixed the problems, run command =condor_release -all= and Condor will check all
held jobs again and change their status accordingly.

*** Condor is copying unwanted files to my machine, how can I avoid that? 

By default, Condor will copy all files generated or modified by your
application that are located in the same directory where your program was
executed on the remote machine, what could include some unwanted content like
temporary files, etc. If you want to avoid that, then you can use the
=transfer_output_files= command (see *this FAQ -> #outputs*) to specify which
files and/or directories you want that Condor copies from the remote machine to
your machine once your application has finished (then Condor will copy {+only+}
those files and ignore all remaining ones).

If you only want to get the output file from the screen (using =output=
command), but not any other generated of modified file, you can use
=should_transfer_files = NO= command. That command will deactivate the Condor
transfer mechanism, affecting both your input and output files, so it can be
only used when you have none of them. If you want to copy input files, but NOT
the output files, then you should use next commands:

 =should_transfer_files=  = YES
 =+TransferOutput=        = ""

*** Some of my jobs are failing due to wrong inputs, can I fix that problem and then run again only those jobs that failed?  

First of all, we strongly recommend you always perform some simple tests
before submitting your actual jobs in order to make sure that both your submit
file and program work in a proper way: if you are going to submit hundreds of
jobs and each job takes several hours to finish, before doing that try with just
a few jobs and change the input data in order to let them finish in
minutes. Then check the results to see if everything went fine, and if so, then
submit your real jobs. Bear in mind that submitting untested files and/or jobs
may cause a waste of time and resources if they fail, and also your priority
will be worse in following submissions.

Sometimes we discover too late that there were some problems, most times related
to the executable and/or the input files. If some of the jobs have run correctly
while others have failed, we will try to fix the problems and execute again only
those that have failed, to avoid wasting time and resources executing again jobs
that worked fine. For the same reason, we should stop as soon as possible all
those running (or idle jobs) that will fail. Every submission is different, and
it is not possible to give general advice, but next steps should help you (and
you can always contact us to study your particular situation):

+ *Identify those jobs that failed*: if your queue only contains failing jobs
  since all correct ones have already finished, then it will very easy to manage
  them. But most times you will have different jobs in your queue: correct ones
  that are running, incorrect ones also running, some of them that are held,
  others that are idle so we do not know whether they are correct or not,
  etc. The first thing we have to do is to find an expression to identify all
  failing jobs. Usually when jobs fail there is a way to recognize them, for
  example, they have been executing for a very long time (many hours when they
  only need a few minutes to finish), or very short, or the exit code is not the
  expected one, etc. Use =condor_q= command to list them with =-constraint=
  option and an expression, we will give you some tips to find those jobs:

  + All held jobs (=JobStatus == 5=) from Cluster with ID 453 (=ClusterID == 453=)
    are not correct. To list them we simply use next command:

     =condor_q= -constraint '((JobStatus == 5) && (ClusterID == 453))'
  + Our jobs need less than 10 minutes to finish, those that are running
    (=JobStatus == 2=) for more than 30 minutes (=(CurrentTime - JobStartDate) >
    1800=) are not correct. Then we can list them using next command:

      =condor_q= -constraint '((JobStatus == 2) && ((CurrentTime - JobStartDate) > 1800))'
  + All those idle jobs (=JobStatus == 1=) that have been running for more than 2
    hours (=CumulativeSlotTime > 7200=) are wrong:

      =condor_q= -constraint '((JobStatus == 1) && (CumulativeSlotTime > 7200))'

    Note the difference between ''cumulative time'' (the sum of the time consumed
    in different executions if the job have been evicted) and the ''consumed
    time'' of the present execution. To see the consumed time of all running jobs
    you can use =condor_q -run -currentrun= (or use =-cputime= to see the real CPU
    time consumed without being suspended), and you can also use
    =condor_ssh_to_job= to connect and see what is happening (check *this FAQ ->
    #ssh*).
  + Jobs have many other attributes that can be used in the constraints, just
    choose a incorrect job (for example job with ID =XXX.YYY=) and run =condor_q
    -long XXX.YYY= to get all attributes of that job. Then try to find which
    attributes can be used to identify all wrong jobs. All valid =JobStatus= could
    be displayed using shell command: =condor_q -help status=


+ *Stop all failing jobs and run them again with correct data*: Once you have
  all your failing jobs listed and the problem with input if fixed, we will try
  to stop those wrong jobs and re-execute them with the right data. There are
  two situations, depending on how you solved the problem:

  + ''Situation A: to solve the problem you only need to correct the executable
    and/or the input files, but the submit files was NOT changed.'' This is the
    easiest situation, you have to be sure that the new input files are in the
    same location that they were previously and exactly with the same names. Then
    you only need to hold all those wrong jobs and release them again, so the new
    executable and input files will be copied. To do it, use the same expression
    you had before to list the jobs, but change command to =condor_hold=:

      =condor_q= -constraint '(=XXX=)'      ''List all failing jobs, =XXX= is the expression to identify them''
      =condor_hold= -constraint '(=XXX=)'   ''Hold all failing jobs''
      =condor_release= -all               ''Execute again all held jobs (we assume that all held jobs are those failing,''
                                        ''if not, just find and use a =-constraint= expression)''

    And that should be all, now all released jobs will have the correct input
    files, so executions should go fine.

  + ''Situation B: to solve the problem you need to change the submit
    file''. Sometimes we cannot avoid changing the submit file because we have to
    modify the commands to add or remove input files, change the arguments,
    etc. In those situations, holding and releasing failing jobs will not work
    because the submit file is only processed at the submission time. Then we need
    to use =*condor_qedit*= (see *this FAQ -> #ch_submit*) to change the values of
    the attributes specified in the submit file; or you can also remove the wrong
    jobs and submit them again. For the last option, simply follow next steps (we
    are assuming here that all jobs belong to the same Cluster, if you have done
    several submission, then you will have to repeat these steps several times):

    + Get the list of Process ID of all failing jobs (use the same expression
      (=XXX=) that you get in the first step):

       =condor_q= -constraint '(=XXX=)' -format "%d," ProcID

      For example, assume that the output of the last command is =0,4,67,89,245,=

    + Remove all those failing jobs:
        =condor_rm= -constraint '(=XXX=)'

    + Change your submit file as needed and add next command to only execute the
      failing jobs 

        =noop_job= = =*!*stringListMember=("$(Process)","0,4,67,89,245")

      Important!! When re-submitting, output, log, and error files of ALL jobs (even
      those correct ones) may be overwritten, so save the old ones if they are
      important.

    + Submit again.

       =condor_submit= your_submit_file

    + Jobs that are not in the =noop_job= list will not be executed, but they may
      stay in the queue with =Complete= status, use next command to remove them from
      the queue (read *this FAQ -> #repeat* for more info)

        =condor_rm= -constraint 'JobStatus == 4'

  As you can see, the steps to follow strongly depend on each particular problem,
  so it might be easier if you just come to our office. 



*** Some of my jobs randomly fail (or I know they will fail in some specific machines)... how can I prevent that? 

If you see that some of your jobs fail with apparently no reasons, but they
properly run when resubmitted, the problem might not be in your program, but on
the machine(s) where they were executed (for example, an application or library
that is used by your program is not installed on those machines, or its version
is too old/new, or it is misconfigured, etc.). To detect this, simply check the
machine where the failing job was executed, which is written in your condor log
file, though it is easier to check it using the =condor_history= command. For
instance, to check where job =XXX.YYY= was run, launch next command in the
{+same machine+} where you did the submission:

 [...]$ =condor_history= XXX.YYY =-af= LastRemoteHost

Maybe some of your jobs finished with no problems, but others finished
abnormally soon. You can use =condor_history= to get a list of those jobs. For
instance, suppose that you have submitted some jobs with =clusterId=XXX= and
each job needs at least 30 minutes to properly finish, so you are sure that
those that lasted less than 10 minutes (600 seconds) failed. Then you can use
next commands to get those jobs (first command will give you a list of the jobs
that failed and the second one will show two lines for each of them, the first
line is where the jobs was executed on and the second line is the =procId= of
the job):

 [...]$ =condor_history= -constraint '((ClusterId==XXX) && ((CompletionDate-JobStartDate) < 600))'
 [...]$ =condor_history= -constraint '((ClusterId==XXX) && ((CompletionDate-JobStartDate) < 600))' =-af= ProcId LastRemoteHost


Most times these problems are simply solved by forcing these failing jobs to go
again into the queue after an unsuccessful execution to be re-executed (see last
paragraph). If you see that all jobs that failed were executed on the same
machine(s) or you already know that your application is not able to run on some
machines, then you can force Condor to avoid sending your jobs to those
machines. For instance, suppose that your jobs have problems in machines with
names ''agora'', ''lapiz'' and ''madera'' and you want to avoid them. Then, add
either of the next lines (both are equivalent) to your Condor submit file (if
you had some previous requirements, append the new ones to them):

 =requirements= = ((UtsnameNodename =!= "agora") && (UtsnameNodename =!= "lapiz") && (UtsnameNodename =!= "madera"))
 =requirements= = *!*=stringListMember=(UtsnameNodename, "agora,lapiz,madera")

You can also block all machines that satisfy a pattern. For instance, to avoid
executing your jobs in those machines with names beginning with "a", "l" and
"m", add next lines (you can specify more complex patterns using the *predefined
functions and macros ->
http://research.cs.wisc.edu/htcondor/manual/v8.6/4_1HTCondor_s_ClassAd.html#SECTION00512400000000000000*):

 letter       = =substr=(toLower(Target.Machine),0,1)
 =requirements= = *!*=stringListMember=($(letter), "a,l,m")

On the opposite situation, if your application can ONLY run on those machines,
then you only need to negate the previous expressions (or remove the negation):

 =requirements= = ((UtsnameNodename == "agora") || (UtsnameNodename == "lapiz") || (UtsnameNodename == "madera"))  
 =requirements= = =stringListMember=(UtsnameNodename, "agora,lapiz,madera")
 ...
 letter       = =substr=(toLower(Target.Machine),0,1)
 =requirements= = =stringListMember=($(letter), "a,l,m")


Then you should execute again only {+those jobs that failed+} (check *this FAQ
-> #repeat* to see how). Please, do not execute again all your jobs to avoid
wasting time and resources. If your program could fail and never end (for
example, for some sets of data it never converges), you can use utilities like
linux command =timeout= to limit the time it can be running. Failing machines
can cause a problem called *black hole* that could produce that most of your
jobs fail. Please, visit *Condor submit file section ->
CondorHowTo#howto_failing* for more info and examples to avoid that. In this
section we also describe some Condor commands that you can add in your submit
file to deal with failing machines, like =on_exit_hold= and
=on_exit_remove=. For instance, using these commands you can specify that any
job that finishes with a non valid exit code and/or before X minutes, has to be
held or sent to the queue again to be re-executed, respectively. Some examples
(before using these commands, make sure that the problem is on remote machines
and not on your code in order to avoid re-executing failing jobs):

 [--# Held jobs if they finished in less than 10 minutes. Later we can check what was wrong with those jobs and re-execute again them--]
 [--# using =condor_release= (we can also use =periodic_release= to automatically release held jobs every X minutes)--]
 =on_exit_hold= = ((CurrentTime - JobStartDate) < (10 * 60)

 [--# Remove from the queue only those jobs that finished after 10 or more minutes. If a job finished before that period of time,--]
 [--# it will be sent again to the queue with 'Idle' status to be re-executed (most probably on a different machine)--]
 =on_exit_remove= = ((CurrentTime - JobStartDate) > (10 * 60)


*** I want to repeat (resubmit) ONLY some of my jobs, is that possible? 

If you submit a large number of jobs and for any reason some of them fail
and leave the queue, you should not waste time and resources running again all
of them, just try with those that failed (after solving the problems they
had). Unfortunately there is *not* a =condor_resubmit= command to easily
resubmit jobs that have already left the queue. You could try to obtain the
''ClassAd'' of those jobs using =condor_history -l <job.id>=, but Condor will
not accept it as input when using =condor_submit=.

If there are just a few jobs to resubmit, you could try to add pairs of
=arguments= and =queue= commands to execute only those jobs, but there is an
easier way to do it using =noop_job= command. For instance, suppose you want to
repeat jobs with Process ID =0=, =4=, =9=, =14= and those from =24= to
=32=. Then, add next line to your submit file and submit it again:

  =noop_job= = =*!*( stringListMember=("$(Process)","0,4,9,14") || (($(Process) >= 24) && ($(Process) <= 32)) =)=

Condor will *not* run jobs where that expression is =True=, so only jobs in the
list will be executed. Note that we have added an exclamation mark symbol
(=*!*=) before your expression to change its value: =noop_job = *!*(...)=. When
using =noop_job=, Condor will still create output and error files for all jobs,
but they will be empty for those jobs that will not be executed (be careful to
avoid that new executions overwrite output files of previous ones).

Jobs that are not executed may stay in the queue with =Complete= status (when
using =condor_q= you will see that =ST= column is =C=). To remove all =C= jobs
from the queue, try next command in your shell (use the second one to only
remove =Complete= jobs that belongs to cluster =XXX=):

  =condor_rm= -constraint 'JobStatus == 4'
  =condor_rm= -constraint 'JobStatus == 4 && clusterID == XXX'

*** I see that my jobs complete after being running for N+X minutes/hours, when they only need N to finish. Is that normal? 

Yes, it is normal. Bear in mind that executing a Condor job in a machine is
only possible when it is not used by its owner. If Condor detects any user's
activity in a machine when executing jobs, they will be suspended or moved to
another machines, increasing the consumed time (and that may happens several
times, so the extra time could be quite long).

Condor has several ways to show the time that jobs have been running. If you use
=condor_q=, the time showed is the cumulative one by default (the result of
adding the time consumed in all executions), so it could be really high if the
job has been killed and restarted several times. If you use =-currentrun=
option, then Condor will only display the time consumed in the current
execution, which is a more realistic time (although if the job has been
suspended, that time is also included). You can also use =-cputime= option to
get only the CPU time (but if the job is currently running, time accumulated
during the current run is not shown).

If your jobs finish in a reasonable amount of time, everything is fine. If they
never finish or need an excessive amount of time to complete, you will have to
modify the application to create checkpoints.

*** I have submitted jobs that need some hours to finish. They have been running for days and just few have finished... what is happening? 

First of all, check that your program is properly running. Maybe there are
some problems with the data, input files, etc. You can open a shell and check
the running job using the =condor_ssh_to_job= command (see *this FAQ ->
##ssh*). If you discover that there are some problems with your job and it will
not produce valid results, you should stop it as soon as possible to avoid
wasting more time and resources, see *this FAQ -> #bad_inputs* for more
details. If your job is working fine, maybe your job has been killed and
restarted several times. Condor shows the ''cumulative running time'' by
default, you can see the consumed time of the present execution using =condor_q
-run -currentrun= command.

The reason why Condor kill and restart jobs is that it has several runtime
environments called ''universes''. By default, all your jobs will go to the most
basic (also the simplest) one called =vanilla= universe. In that universe, when
Condor detects that a machine is not idle anymore, it will suspend all running
jobs for a while, waiting the machine to get idle again. If that does not happen
in a given (short) time interval, then Condor will kill all jobs and send them
again to the queue with ''Idle'' status, so those jobs will start from the
beginning. If your jobs need some hours to finish, probably some of them will be
killed before their completion and restarted in other machines, that could
happen even several times.

However, most times we can solve this problem simply changing the arguments of
our jobs. For instance, suppose you have to process 10,000 inputs and each input
needs about 2 minutes to be done. You can create 100 jobs to process 100 inputs
each, but they will need more than 3 hours to finish and it is likely they will
be killed several times. It is better to choose faster jobs that can be finished
in about 15-30 minutes so they will have more possibilities to be processed on
the same machine without being killed and restarted on other machines. If you
choose that each job works with 10 inputs, then you will have 1000 jobs and they
will need about 20 minutes to finish, that could be a good approach.

*** Some of my python programs work fine, but other fail... 

If you are executing python programs with HTCondor and some jobs work fine
and other fail, most probably you are experiencing problems related to the
version of Fedora. Most of the old Linux Desktop PCs have installed Fedora21,
but newer machines have a more recent version, mostly Fedora26 (although we also
have a few with Fedora25). Paths to python libraries are different on the old
and new machines, therefore your programs will only work properly on those
machines that have the same Fedora version as the machine where you have
submitted the jobs.

To fix this issue, you can force HTCondor to only execute jobs on machines with
your same version of Fedora, then your environment variables and paths will
work. For instance, if you are working on a machine with Fedora21, add the next
requirement to force that all your jobs will be executed on machines running
Fedora21:

  =*requirements* =  (*OpSysMajorVer* == *21*)=

But adding that requirement will limit the number of available machines to
execute your program: if you only run on machines with Fedora21, you will be
missing all new and faster machines, and if you only run on machines with
Fedora26, then you will be losing a big amount of slots since still most of the
machines run Fedora21. We recommend you change a bit your submit script to be
able to run your python programs on all machines, independently of their O.S (we
will only avoid the few machines beginning with ='f'=, since they have a special
purpose and python installation there is not the usual one):

  ... 
  =transfer_input_files= = your_program.py

  =getenv=       = =True=
  =environment=  = "PYTHONPATH=/usr/pkg/python/python2.7/lib/python2.7/site-packages"
  =requirements= = (*!*=stringListMember=(=substr=(=toLower=(=Target.Machine=),0,1), "f"))

  =transfer_executable= = =False=
  =executable=   = /usr/bin/env
  =arguments=    = =python= your_program.py

  =queue= ...

Example above is just a basic one, you might need to adapt it adding some other
commands to transfer your input/output files, add requirements, etc., and, of
course, all common commands (see *common template ->
HOWTOs.CondorSubmitFile#common_template*).  Contact us if you have any doubts.


*** I receive an error when running HTCondor jobs that use python and matplotlib... 

If you are running some =python= jobs that use =matplotlib= (for example,
 to make some plots and save them to =png= images) and receive errors like:

+ =no display name and no $DISPLAY environment variable=
+ =: cannot connect to X server :0=

it might be caused because =matplotlib= (and/or some other packages) needs a
=DISPLAY= environment variable, which means you have to execute it in a X
server, and that is not available when running on HTCondor. In this case, simply
use another background that does not need a X server, like =Agg=. For instance,
you can adapt next python code when using =matplotlib=:

  =import= matplotlib =as= mpl
  mpl.use(*'Agg*')
  =import= matplotlib.pyplot =as= plt

  ''# Now use =plt= as usual''
  ''...''
  ''fig = plt.figure()''
  ''...''
  ''fig.savefig('image.png')''
  ''...''

You can find more info about this issue *here ->
http://stackoverflow.com/questions/4931376/generating-matplotlib-graphs-without-a-running-x-server*.

*** I would like to get more information about the execution, is there an easy way to see the logs created by Condor? 

Yes, there are several possibilities for that. The first step is to create
the ''condor log file'' adding the next command to your submit file:

  =log= = file.log      ''#(we recommend you use =your_executable_name.$(Cluster).log= as name for your log file)''

Once you have your condor log file, you can display the information using the
following options:

+ Directly check the content of the condor log file with any text editor (not
  recommended)
+ Use =condor_userlog <file.log>= to get a summary of the execution.
+ Run =condor_history -userlog <file.log>= command in your shell to list basic
  information contained in the log file.
+ Use =condor_logview <file.log>= to open the ''Condor log viewer'' and see more
  detailed information in graphical mode, showing the timeline of your jobs and
  allowing you to perform zooms, filter jobs, etc.
+ There is also an online tool to analyze your log files and get more
  information: ''Condor Log Analyzer'' (*http://condorlog.cse.nd.edu/*).

If you just want some general information about Condor queue, the pool of
machines, where jobs have been executed on, etc., you can also try our online
stats about Condor: *http://carlota:81/condor_stats/* and *nectarino ->
http://nectarino/*.



** Special needs
*** I am running many jobs, but some are more important than others, how can I prioritize them? 

You can prioritize your jobs (and only your jobs, not other users' jobs)
using =priority = <value>= command in your submit files (the higher value, the
better priority). Once you have submitted your jobs, you can check or modify
their priority by running =condor_prio= in a console. Please, check *Condor
submit file page -> CondorHowTo#howto_priority* to see more examples, and also
*this FAQ -> #idle* for more info about users' priorities.

*** I am receiving hundreds of emails from Condor, can I stop that? 

Yes, by default Condor send an email notifying any event related to each
job (termination, errors, etc.). If you launch 1000 jobs, that could be really
annoying. To avoid that, use next command in your submit file: =notification =
Never= (use =Complete= if you only want to know when they finish, =Error= when
they fail or =Always= to receive all notifications; we recommend you use
=Error=). Also you can change the email address using =notify_user = <email>=.

''Please, visit *Condor submit file page -> CondorSubmitFile* for more info and
examples.''

*** What happens with my IDL or Matlab jobs that require licences to run? 

There is a limited number of IDL licences, so if you try to run a large
number of IDL jobs they could fail since there may not be enough licences. But
using IDL Virtual Machine does not consume any licence, so there will not be
limit in the number of simultaneous IDL running jobs, just the number of
available slots. See *detailed information here -> CondorAndIDLVirtualMachine*.

There is a similar limitation with Matlab licences, that could be saved if it is
possible for you to create Matlab executables using the *Matlab Compiler ->
http://www.mathworks.es/products/compiler/*. You have more info about this topic
*here -> https://htcondor-wiki.cs.wisc.edu/index.cgi/wiki?p=HowToRunMatlab*.




















